import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import json
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import seaborn as sns
import glob
import pickle
from sklearn.metrics import average_precision_score, precision_recall_curve


# ì•ì„œ ìƒì„±í•œ ëª¨ë“ˆë“¤ import
sys.path.append('.')
from gk2a_preprocessor import GK2ACloudProcessor
from cloud_convlstm_model import ImprovedCloudMovementPredictor, CloudDataset, improved_train_model

def binary_iou(pred_mask: torch.Tensor, true_mask: torch.Tensor, eps: float = 1e-7) -> float:
    """
    pred_mask, true_mask: (B, 1, H, W) ë˜ëŠ” (B, H, W), ê°’ì€ 0/1
    """
    if pred_mask.dim() == 4 and pred_mask.size(1) == 1:
        pred_mask = pred_mask.squeeze(1)
    if true_mask.dim() == 4 and true_mask.size(1) == 1:
        true_mask = true_mask.squeeze(1)

    intersection = (pred_mask * true_mask).float().sum(dim=(1, 2))
    union = (pred_mask + true_mask - pred_mask * true_mask).float().sum(dim=(1, 2))
    iou = (intersection + eps) / (union + eps)  # (B,)
    return iou.mean().item()

def compute_pr_auc_from_probs(all_probs: np.ndarray, all_targets: np.ndarray) -> float:
    """
    all_probs: (N,) í™•ë¥ (ì‹œê·¸ëª¨ì´ë“œ í›„)
    all_targets: (N,) 0/1 ë¼ë²¨
    ë°˜í™˜: í‰ê· ì •ë°€ë„(AP) = PR-AUC
    """
    # scikit-learnì˜ average_precision_scoreëŠ” PR-AUC(AP)ë¥¼ ë°˜í™˜
    return float(average_precision_score(all_targets, all_probs))

class CloudPredictionTrainer:
    """êµ¬ë¦„ ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ í´ë˜ìŠ¤"""

    def __init__(self, data_folder="./processed_cloud_data", model_save_dir="./models"):
        self.data_folder = data_folder
        self.model_save_dir = model_save_dir
        os.makedirs(model_save_dir, exist_ok=True)

    def prepare_training_data(self, train_ratio=0.8):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„°ì—ì„œ í›ˆë ¨ ë°ì´í„° ì¤€ë¹„"""
        print("ğŸ“Š ì „ì²˜ë¦¬ëœ ë°ì´í„°ì—ì„œ í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

        # ì „ì²˜ë¦¬ëœ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
        processed_files = glob.glob(f"{self.data_folder}/*_processed.pkl")
        processed_files = sorted(processed_files)

        if len(processed_files) < 10:
            raise ValueError(f"ì¶©ë¶„í•œ ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìµœì†Œ 10ê°œ íŒŒì¼ í•„ìš”, í˜„ì¬: {len(processed_files)}ê°œ")

        print(f"âœ… ì´ {len(processed_files)}ê°œ ì „ì²˜ë¦¬ëœ íŒŒì¼ ë°œê²¬")

        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
        processed_data = []
        valid_files = []

        for i, filepath in enumerate(processed_files):
            try:
                with open(filepath, 'rb') as f:
                    data = pickle.load(f)
                
                processed_data.append({
                    'filepath': filepath,
                    'binary_mask': data['binary_mask'],
                    'patches': data['patches'],
                    'positions': data['positions'],
                    'timestamp': data.get('metadata', {}).get('time', ''),
                    'filename': data['filename']
                })
                valid_files.append(filepath)

                if (i + 1) % 10 == 0:
                    print(f"ë¡œë”© ì§„í–‰ë¥ : {i+1}/{len(processed_files)} ({((i+1)/len(processed_files)*100):.1f}%)")

            except Exception as e:
                print(f"âš ï¸  íŒŒì¼ ë¡œë”© ì‹¤íŒ¨ {filepath}: {e}")
                continue

        print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(processed_data)}ê°œ ì‹œê°„ëŒ€ ë°ì´í„°")

        # í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í• 
        split_idx = int(len(processed_data) * train_ratio)
        train_data = processed_data[:split_idx]
        val_data = processed_data[split_idx:]

        return train_data, val_data, valid_files

    def create_data_loaders(self, train_data, val_data, batch_size=32, sequence_length=4):
        """ë°ì´í„° ë¡œë” ìƒì„±"""
        print("ğŸ”„ ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")

        train_dataset = CloudDataset(
            [d['filepath'] for d in train_data], 
            sequence_length=sequence_length
        )
        val_dataset = CloudDataset(
            [d['filepath'] for d in val_data], 
            sequence_length=sequence_length
        )

        train_loader = torch.utils.data.DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True, num_workers=4
        )
        val_loader = torch.utils.data.DataLoader(
            val_dataset, batch_size=batch_size, shuffle=False, num_workers=4
        )

        print(f"âœ… í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}ê°œ, ê²€ì¦ ë°ì´í„°: {len(val_dataset)}ê°œ")

        return train_loader, val_loader


    def train_and_evaluate(self, train_loader, val_loader, num_epochs=50, device=None,
                       resume=True, best_ckpt_path=None):
        """
        - ì‹¤í–‰ ì‹œì ì— 'ë² ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜(state_dict)ë§Œ' ë¡œë“œ(ìˆìœ¼ë©´).
        - ì˜µí‹°ë§ˆì´ì €/ìŠ¤ì¼€ì¤„ëŸ¬/ì—í­ì€ ë¡œë“œí•˜ì§€ ì•ŠìŒ â†’ í•­ìƒ Epoch 1ë¶€í„° ì‹œì‘.
        """
        print("ğŸš€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")

        model = ImprovedCloudMovementPredictor(
            input_size=(256, 256),
            sequence_length=4,
            prediction_steps=1
        )

        if device is None:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
        if torch.cuda.is_available():
            print(f"ğŸš€ GPU: {torch.cuda.get_device_name(0)}")
            print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")

        model = model.to(device)

        # âœ… ë² ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ ë¡œë“œ (ìˆìœ¼ë©´)
        if best_ckpt_path is None:
            best_ckpt_path = os.path.join(self.model_save_dir, 'best_cloud_model.pth')
        if resume and os.path.exists(best_ckpt_path):
            try:
                state = torch.load(best_ckpt_path, map_location=device)
                # stateê°€ state_dictë¼ê³  ê°€ì • (ìš°ë¦¬ê°€ ê·¸ í˜•íƒœë¡œë§Œ ì €ì¥)
                model.load_state_dict(state, strict=False)
                print(f"âœ… ë² ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ: {best_ckpt_path}")
            except Exception as e:
                print(f"âš ï¸ ë² ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨(ìƒˆë¡œ í•™ìŠµ): {e}")

        # --- í•™ìŠµ(í•­ìƒ Epoch 1ë¶€í„°) ---
        train_losses, val_losses = improved_train_model(
            model, train_loader, val_loader,
            num_epochs=num_epochs, device=device
        )

        # --- í›ˆë ¨ ê³¡ì„  ì €ì¥/í‘œì‹œ ---
        self.plot_training_history(train_losses, val_losses)

        # --- ë§ˆì§€ë§‰ ëª¨ë¸ë„ ì €ì¥(ì°¸ê³ ìš©) ---
        final_model_path = os.path.join(self.model_save_dir, 'final_cloud_model.pth')
        torch.save(model.state_dict(), final_model_path)
        print(f"âœ… í›ˆë ¨ ì™„ë£Œ! ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")

        return model, train_losses, val_losses


    def evaluate_model(self, model, test_loader, device):
        """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")

        model.eval()

        batch_ious = []
        all_probs_list = []
        all_targets_list = []

        # ë¶„ë¥˜ ì§€í‘œ(accuracy/F1 ë“±)ë¥¼ ìœ„í•œ ì´ì§„ ì˜ˆì¸¡/ì •ë‹µ
        all_bin_preds = []
        all_bin_tgts = []

        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)   # target: (B,1,H,W) ê°€ì •
                output = model(data)                                # (B,1,H,W) ë¡œì§“
                proba = torch.sigmoid(output)                       # (B,1,H,W) í™•ë¥ 

                # --- mIoU ê³„ì‚°(ì„ê³„ê°’ 0.5ë¡œ ì´ì§„í™”) ---
                pred_mask = (proba > 0.5).float()                   # (B,1,H,W)
                iou = binary_iou(pred_mask, target)                 # ë‚´ë¶€ì—ì„œ squeeze ì²˜ë¦¬
                batch_ious.append(iou)

                # --- PR-AUC ê³„ì‚°ì„ ìœ„í•œ í™•ë¥ /ì •ë‹µ(ë²¡í„°) ì¶•ì  ---
                all_probs_list.append(proba.detach().cpu().numpy().ravel())
                all_targets_list.append(target.detach().cpu().numpy().ravel())

                # --- ë¶„ë¥˜ ì§€í‘œ(accuracy/F1 ë“±)ìš© ì´ì§„ ë²¡í„°ë„ í•¨ê»˜ ì¶•ì  ---
                all_bin_preds.append((proba > 0.5).float().cpu().numpy().ravel())
                all_bin_tgts.append(target.float().cpu().numpy().ravel())

        # --- ì§‘ê³„ ---
        mean_iou = float(np.mean(batch_ious)) if batch_ious else 0.0

        all_probs   = np.concatenate(all_probs_list, axis=0)
        all_targets = np.concatenate(all_targets_list, axis=0).astype(np.int32)

        pr_auc = compute_pr_auc_from_probs(all_probs, all_targets)

        # ì´ì§„ ì§€í‘œ ê³„ì‚°ìš© (0/1)
        all_preds_bin = np.concatenate(all_bin_preds, axis=0).astype(np.int32)
        all_tgts_bin  = np.concatenate(all_bin_tgts, axis=0).astype(np.int32)

        accuracy = accuracy_score(all_tgts_bin, all_preds_bin)
        precision, recall, f1, _ = precision_recall_fscore_support(
            all_tgts_bin, all_preds_bin, average='binary', zero_division=0
        )
        cm = confusion_matrix(all_tgts_bin, all_preds_bin)

        # --- ë¡œê·¸ ---
        print(f"[Eval] mIoU@0.5 = {mean_iou:.4f} | PR-AUC = {pr_auc:.4f}")
        print(f"   ì •í™•ë„ (Accuracy): {accuracy:.4f}")
        print(f"   ì •ë°€ë„ (Precision): {precision:.4f}")
        print(f"   ì¬í˜„ìœ¨ (Recall): {recall:.4f}")
        print(f"   F1-Score: {f1:.4f}")

        results = {
            'mIoU@0.5': mean_iou,
            'PR-AUC': pr_auc,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'confusion_matrix': cm.tolist()
        }
        return results


    def plot_training_history(self, train_losses, val_losses):
        """í›ˆë ¨ ê³¼ì • ì‹œê°í™”"""
        plt.figure(figsize=(10, 6))
        plt.plot(train_losses, label='Train Loss', color='blue')
        plt.plot(val_losses, label='Validation Loss', color='red')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training History - Cloud Movement Prediction')
        plt.legend()
        plt.grid(True)

        save_path = os.path.join(self.model_save_dir, 'training_history.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

        print(f"âœ… í›ˆë ¨ ê³¼ì • ê·¸ë˜í”„ ì €ì¥: {save_path}")

def print_gpu_info():
    """GPU ì •ë³´ ì¶œë ¥"""
    print("ğŸ” GPU ì •ë³´ í™•ì¸:")
    print(f"   CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   CUDA ë²„ì „: {torch.version.cuda}")
        print(f"   GPU ê°œìˆ˜: {torch.cuda.device_count()}")
        print(f"   GPU ì´ë¦„: {torch.cuda.get_device_name(0)}")
        print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("   GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def run_full_training_pipeline():
    print("ğŸš€ êµ¬ë¦„ ì´ë™ ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹œì‘ (GPU ìµœì í™” ë²„ì „)")
    print("=" * 60)

    # GPU ì •ë³´ ì¶œë ¥
    print_gpu_info()
    print()

    # GPU/CPUì— ë”°ë¥¸ ì„¤ì •
    if torch.cuda.is_available():
        device = torch.device('cuda:0')
        batch_size = 16  # GPUìš© í° ë°°ì¹˜ í¬ê¸°
        num_workers = 4
        print("ğŸš€ GPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    else:
        device = torch.device('cpu')
        batch_size = 4  # CPUìš© ì‘ì€ ë°°ì¹˜ í¬ê¸°
        num_workers = 2
        print("âš ï¸  CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")

    trainer = CloudPredictionTrainer()

    try:
        print("ğŸ“ ì „ì²˜ë¦¬ëœ ë°ì´í„° í™•ì¸ ì¤‘...")
        
        if not os.path.exists('./processed_cloud_data'):
            print("âŒ processed_cloud_data í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤!")
            print("ë¨¼ì € ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return

        processed_files = glob.glob('./processed_cloud_data/*_processed.pkl')
        if len(processed_files) < 10:
            print(f"âš ï¸  ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. í˜„ì¬: {len(processed_files)}ê°œ")
            print("ë” ë§ì€ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê±°ë‚˜, ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”.")
            return

        # ì‹¤ì œ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¡œ í•™ìŠµ
        train_data, val_data, valid_files = trainer.prepare_training_data(train_ratio=0.8)
        train_loader, val_loader = trainer.create_data_loaders(
            train_data, val_data, 
            batch_size=batch_size, 
            sequence_length=4
        )

        model, train_losses, val_losses = trainer.train_and_evaluate(
            train_loader, val_loader, 
            num_epochs=50,
            device=device
        )

        print("âœ… í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")

    except Exception as e:
        print(f"âŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return

if __name__ == "__main__":
    run_full_training_pipeline()
