import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pickle
import os
from datetime import datetime

# 1. ìˆ˜ì •ëœ CloudDataset - ì‹¤ì œ ë°ì´í„° ë¡œë”©
class CloudDataset(Dataset):
    """ì™„ì „íˆ ìˆ˜ì •ëœ CloudDataset - ë”ë¯¸ ë°ì´í„° ì™„ì „ ì œê±°"""
    
    def __init__(self, data_paths, sequence_length=4, prediction_steps=1):
        self.data_paths = sorted(data_paths)
        self.sequence_length = sequence_length
        self.prediction_steps = prediction_steps
        
        # ğŸ¯ í•µì‹¬: ìœ íš¨í•œ ì‹œí€€ìŠ¤ë§Œ ë¯¸ë¦¬ ìƒì„±
        self.valid_sequences = self._create_valid_sequences()
        print(f"âœ… ìœ íš¨í•œ ì‹œí€€ìŠ¤: {len(self.valid_sequences)}ê°œ ìƒì„±ë¨")
        
    def _create_valid_sequences(self):
        """ìœ íš¨í•œ ì‹œí€€ìŠ¤ë§Œ ìƒì„± (ë”ë¯¸ ë°ì´í„° ì™„ì „ ë°°ì œ)"""
        valid_sequences = []
        
        for i in range(len(self.data_paths) - self.sequence_length - self.prediction_steps + 1):
            input_paths = self.data_paths[i:i + self.sequence_length]
            target_paths = self.data_paths[i + self.sequence_length:i + self.sequence_length + self.prediction_steps]
            
            # ğŸ” ë°ì´í„° ìœ íš¨ì„± ì‚¬ì „ ê²€ì¦
            all_valid = True
            
            # ì…ë ¥ ë°ì´í„° ê²€ì¦
            for path in input_paths:
                if not self._is_valid_data_file(path):
                    all_valid = False
                    break
            
            # íƒ€ê²Ÿ ë°ì´í„° ê²€ì¦
            if all_valid:
                for path in target_paths:
                    if not self._is_valid_data_file(path):
                        all_valid = False
                        break
            
            # âœ… ëª¨ë“  íŒŒì¼ì´ ìœ íš¨í•œ ê²½ìš°ì—ë§Œ ì‹œí€€ìŠ¤ ì¶”ê°€
            if all_valid:
                valid_sequences.append((input_paths, target_paths))
            else:
                print(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ì‹œí€€ìŠ¤ ì œì™¸: {input_paths[0]} ~ {target_paths[-1]}")
        
        return valid_sequences
    
    def _is_valid_data_file(self, path):
        """ë°ì´í„° íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            with open(path, 'rb') as f:
                data = pickle.load(f)
                
            # binary_mask ì¡´ì¬ ë° ìœ íš¨ì„± ê²€ì‚¬
            if 'binary_mask' not in data:
                return False
                
            binary_mask = data['binary_mask']
            if binary_mask is None:
                return False
                
            # í¬ê¸° ê²€ì‚¬ (ë„ˆë¬´ ì‘ê±°ë‚˜ ë¹ˆ ë°ì´í„° ì œì™¸)
            if binary_mask.size == 0:
                return False
                
            # ëª¨ë“  ê°’ì´ ë™ì¼í•œ ê²½ìš° ì œì™¸ (ì˜ë¯¸ì—†ëŠ” ë°ì´í„°)
            if np.all(binary_mask == binary_mask.flat[0]):
                return False
                
            return True
            
        except Exception:
            return False
    
    def _load_and_process_data(self, path):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (ë”ë¯¸ ë°ì´í„° ì—†ìŒ)"""
        with open(path, 'rb') as f:
            data = pickle.load(f)
            
        binary_mask = data['binary_mask']
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        binary_mask = np.where(binary_mask == -1, 0, binary_mask)
        
        # ì •ê·œí™” (0-1 ë²”ìœ„)
        if binary_mask.max() > 1:
            binary_mask = binary_mask / binary_mask.max()
        
        # í¬ê¸° ì¡°ì • (256x256)
        binary_mask = self._resize_to_target(binary_mask, (256, 256))
        
        return binary_mask.astype(np.float32)
    
    def _resize_to_target(self, data, target_size=(256, 256)):
        """ë°ì´í„°ë¥¼ ëª©í‘œ í¬ê¸°ë¡œ ì¡°ì •"""
        if data.shape == target_size:
            return data
            
        h, w = data.shape
        target_h, target_w = target_size
        
        if h >= target_h and w >= target_w:
            # ì¤‘ì•™ í¬ë¡­
            start_h = (h - target_h) // 2
            start_w = (w - target_w) // 2
            return data[start_h:start_h+target_h, start_w:start_w+target_w]
        else:
            # íŒ¨ë”©
            pad_h = max(0, (target_h - h) // 2)
            pad_w = max(0, (target_w - w) // 2)
            pad_h_end = target_h - h - pad_h
            pad_w_end = target_w - w - pad_w
            return np.pad(data, ((pad_h, pad_h_end), (pad_w, pad_w_end)), mode='constant')
    
    def __len__(self):
        return len(self.valid_sequences)
    
    def __getitem__(self, idx):
        """ë”ë¯¸ ë°ì´í„° ì—†ëŠ” ì™„ì „í•œ ë°ì´í„° ë¡œë”©"""
        input_paths, target_paths = self.valid_sequences[idx]
        
        # ğŸ¯ ì…ë ¥ ë°ì´í„° ë¡œë”© - ë”ë¯¸ ë°ì´í„° ì—†ìŒ
        input_data = []
        for path in input_paths:
            try:
                processed_data = self._load_and_process_data(path)
                input_data.append(processed_data)
            except Exception as e:
                # ğŸš¨ ë”ë¯¸ ë°ì´í„° ìƒì„± ì—†ìŒ! ëŒ€ì‹  ì—ëŸ¬ ë°œìƒ
                raise RuntimeError(f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {path}, ì—ëŸ¬: {e}")
        
        # ğŸ¯ íƒ€ê²Ÿ ë°ì´í„° ë¡œë”© - ë”ë¯¸ ë°ì´í„° ì—†ìŒ  
        target_data = []
        for path in target_paths:
            try:
                processed_data = self._load_and_process_data(path)
                target_data.append(processed_data)
            except Exception as e:
                # ğŸš¨ ë”ë¯¸ ë°ì´í„° ìƒì„± ì—†ìŒ! ëŒ€ì‹  ì—ëŸ¬ ë°œìƒ
                raise RuntimeError(f"íƒ€ê²Ÿ ë¡œë”© ì‹¤íŒ¨: {path}, ì—ëŸ¬: {e}")
        
        # í…ì„œ ë³€í™˜
        # numpy arrayë¡œ ë¨¼ì € ë³€í™˜ í›„ tensorë¡œ ë³€í™˜
        input_array = np.array(input_data)
        target_array = np.array(target_data) 
        input_tensor = torch.FloatTensor(input_array).unsqueeze(1)
        target_tensor = torch.FloatTensor(target_array).unsqueeze(1)

        
        return input_tensor, target_tensor

class ConvLSTMCell(nn.Module):
    """ConvLSTM Cell êµ¬í˜„"""
    
    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):
        super(ConvLSTMCell, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.kernel_size = kernel_size
        self.padding = kernel_size[0] // 2, kernel_size[1] // 2
        self.bias = bias
        
        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,
                            out_channels=4 * self.hidden_dim,
                            kernel_size=self.kernel_size,
                            padding=self.padding,
                            bias=self.bias)
    
    def _init_hidden(self, batch_size, image_size, device=None):
        height, width = image_size
        if device is None:
            device = next(self.parameters()).device
        
        h = torch.zeros(batch_size, self.hidden_dim, height, width, device=device)
        c = torch.zeros(batch_size, self.hidden_dim, height, width, device=device)
        return h, c
    
    def forward(self, input_tensor, cur_state):
        h_cur, c_cur = cur_state
        
        combined = torch.cat([input_tensor, h_cur], dim=1)
        combined_conv = self.conv(combined)
        
        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)
        i = torch.sigmoid(cc_i)
        f = torch.sigmoid(cc_f)
        o = torch.sigmoid(cc_o)
        g = torch.tanh(cc_g)
        
        c_next = f * c_cur + i * g
        h_next = o * torch.tanh(c_next)
        
        return h_next, c_next


class ConvLSTM(nn.Module):
    """ConvLSTM êµ¬í˜„"""
    
    def __init__(self, input_dim, hidden_dims, kernel_sizes, num_layers, 
                 batch_first=True, bias=True, return_all_layers=False):
        super(ConvLSTM, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.kernel_sizes = kernel_sizes
        self.num_layers = num_layers
        self.batch_first = batch_first
        self.bias = bias
        self.return_all_layers = return_all_layers
        
        cell_list = []
        for i in range(self.num_layers):
            cur_input_dim = self.input_dim if i == 0 else self.hidden_dims[i - 1]
            
            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,
                                        hidden_dim=self.hidden_dims[i],
                                        kernel_size=self.kernel_sizes[i],
                                        bias=self.bias))
        
        self.cell_list = nn.ModuleList(cell_list)
    
    def _init_hidden(self, batch_size, image_size):
        init_states = []
        for i in range(self.num_layers):
            init_states.append(self.cell_list[i]._init_hidden(batch_size, image_size))
        return init_states
    
    def forward(self, input_tensor, hidden_state=None):
        if not self.batch_first:
            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)
        
        b, seq_len, _, h, w = input_tensor.size()
        
        if hidden_state is None:
            hidden_state = self._init_hidden(batch_size=b, image_size=(h, w))
        
        layer_output_list = []
        last_state_list = []
        
        cur_layer_input = input_tensor
        
        for layer_idx in range(self.num_layers):
            h, c = hidden_state[layer_idx]
            output_inner = []
            for t in range(seq_len):
                h, c = self.cell_list[layer_idx](cur_layer_input[:, t, :, :, :], (h, c))
                output_inner.append(h)
            
            layer_output = torch.stack(output_inner, dim=1)
            cur_layer_input = layer_output
            
            layer_output_list.append(layer_output)
            last_state_list.append((h, c))
        
        if not self.return_all_layers:
            layer_output_list = [layer_output_list[-1]]
            last_state_list = [last_state_list[-1]]
        
        return layer_output_list, last_state_list


# 2. ê°œì„ ëœ ëª¨ë¸ ì•„í‚¤í…ì²˜
class ImprovedCloudMovementPredictor(nn.Module):
    """ê°œì„ ëœ êµ¬ë¦„ ì´ë™ ì˜ˆì¸¡ ëª¨ë¸"""
    
    def __init__(self, input_size=(256, 256), sequence_length=4, prediction_steps=1):
        super(ImprovedCloudMovementPredictor, self).__init__()

        
        # ConvLSTM ë ˆì´ì–´ - ë” ì•ˆì •ì ì¸ ì„¤ì •
        self.convlstm = ConvLSTM(
            input_dim=1,
            hidden_dims=[32, 32],  # ì•ˆì •ì ì¸ 2ì¸µ êµ¬ì¡° ìœ ì§€
            kernel_sizes=[(3, 3), (3, 3)],
            num_layers=2,
            batch_first=True,
            bias=True,
            return_all_layers=False
        )
        
        # ë“œë¡­ì•„ì›ƒ ì¶”ê°€
        self.dropout = nn.Dropout2d(0.1)
        
        # ì¶œë ¥ ë ˆì´ì–´ ê°œì„ 
        self.output_layer = nn.Sequential(
            nn.Conv2d(32, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.1),
            
            nn.Conv2d(16, 8, kernel_size=3, padding=1), 
            nn.BatchNorm2d(8),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(8, prediction_steps, kernel_size=1)
            # Sigmoid ì œê±° - BCEWithLogitsLoss ì‚¬ìš©
        )
        
    def forward(self, x):
        lstm_out, _ = self.convlstm(x)
        last_output = lstm_out[0][:, -1, :, :, :]
        
        # ë“œë¡­ì•„ì›ƒ ì ìš©
        last_output = self.dropout(last_output)
        
        prediction = self.output_layer(last_output)
        return prediction

# 3. ê°œì„ ëœ í›ˆë ¨ í•¨ìˆ˜
def improved_train_model(model, train_loader, val_loader, num_epochs=50, device='cuda'):
    """ê°œì„ ëœ ëª¨ë¸ í›ˆë ¨ í•¨ìˆ˜ â€” 'ê°€ì¤‘ì¹˜ë§Œ' ì €ì¥í•˜ì—¬ ë‹¤ìŒ ì‹¤í–‰ì—ì„œ ì—í­ì€ í•­ìƒ 1ë¶€í„° ì‹œì‘"""

    import os
    import torch
    import torch.nn as nn
    import torch.optim as optim

    # 1) ë¶ˆê· í˜• ëŒ€ì‘ (í•„ìš”ì‹œ ê°’ ì¡°ì •: 2.0 -> 5~15)
    pos_weight = torch.tensor([2.0], device=device)
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

    # 2) Optimizer/ìŠ¤ì¼€ì¤„ëŸ¬
    optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)

    train_losses, val_losses = [], []
    best_val_loss = float('inf')
    patience, patience_counter = 15, 0

    # âœ… ì €ì¥ ê²½ë¡œ(ë¡œë”ì™€ í†µì¼): ê°€ì¤‘ì¹˜ë§Œ ì €ì¥
    weights_path = os.path.join("models", "best_cloud_model.pth")
    os.makedirs(os.path.dirname(weights_path), exist_ok=True)

    # (ì„ íƒ) ì°¸ê³ ìš© í’€ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ
    full_ckpt_path = os.path.join("models", "best_cloud_model_full.pth")

    print(f"ğŸš€ í›ˆë ¨ ì‹œì‘ - ë””ë°”ì´ìŠ¤: {device}")

    for epoch in range(num_epochs):
        # -------------------- Train --------------------
        model.train()
        train_loss_sum, train_batches = 0.0, 0

        for batch_idx, (data, target) in enumerate(train_loader):
            try:
                data, target = data.to(device), target.to(device)
                optimizer.zero_grad()

                output = model(data)            # (B,1,H,W)
                target = target.squeeze(1)      # (B,H,W)
                loss = criterion(output, target)

                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

                train_loss_sum += loss.item()
                train_batches += 1

                if batch_idx % 3 == 0:
                    print(f'Epoch {epoch+1:2d}/{num_epochs}, Batch {batch_idx:3d}/{len(train_loader):3d}, Loss: {loss.item():.6f}')
            except RuntimeError as e:
                print(f"âš ï¸ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

        avg_train = train_loss_sum / max(1, train_batches)
        train_losses.append(avg_train)

        # -------------------- Validation --------------------
        model.eval()
        val_loss_sum, val_batches = 0.0, 0
        with torch.no_grad():
            for data, target in val_loader:
                try:
                    data, target = data.to(device), target.to(device)
                    output = model(data)
                    target = target.squeeze(1)
                    loss = criterion(output, target)
                    val_loss_sum += loss.item()
                    val_batches += 1
                except RuntimeError as e:
                    print(f"âš ï¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue

        avg_val = val_loss_sum / max(1, val_batches)
        val_losses.append(avg_val)

        # ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” 'ê²€ì¦ ì†ì‹¤'ë¡œ ì—í­ë§ˆë‹¤ í•œ ë²ˆ
        prev_lr = optimizer.param_groups[0]['lr']
        scheduler.step(avg_val)
        cur_lr = optimizer.param_groups[0]['lr']
        if cur_lr < prev_lr:
            print(f"ğŸ”» LR reduced: {prev_lr:.3e} -> {cur_lr:.3e} (val={avg_val:.6f})")

        print(f'Epoch {epoch+1:2d}/{num_epochs} | ğŸ“‰ Train: {avg_train:.6f} | ğŸ“Š Val: {avg_val:.6f} | ğŸ“ˆ LR: {cur_lr:.3e}')

        # -------------------- Best Save --------------------
        if avg_val < best_val_loss:
            best_val_loss = avg_val
            patience_counter = 0

            # âœ… 1) ê°€ì¤‘ì¹˜(state_dict)ë§Œ ì €ì¥ â€” ë‹¤ìŒ ì‹¤í–‰ì—ì„œ ì—í­ì€ 1ë¶€í„° ì‹œì‘
            torch.save(model.state_dict(), weights_path)
            print(f'  ğŸ’¾ [BEST-weights] ì €ì¥: {weights_path} (val={best_val_loss:.6f})')

            # 2) (ì„ íƒ) ì°¸ê³ ìš© í’€ ì²´í¬í¬ì¸íŠ¸ë„ ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´ ìœ ì§€
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': avg_train,
                'val_loss': avg_val,
            }, full_ckpt_path)
            print(f'  ğŸ§© [BEST-full] ì €ì¥: {full_ckpt_path}')
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f'ğŸ›‘ Early stopping at epoch {epoch+1}')
            break

        print('-' * 50)

    return train_losses, val_losses


# 4. ìˆ˜ì •ëœ CloudPredictionTrainer í´ë˜ìŠ¤
class ImprovedCloudPredictionTrainer:
    """ê°œì„ ëœ êµ¬ë¦„ ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨ í´ë˜ìŠ¤"""
    
    def __init__(self, data_folder="./processed_cloud_data", model_save_dir="./models"):
        self.data_folder = data_folder
        self.model_save_dir = model_save_dir
        os.makedirs(model_save_dir, exist_ok=True)
        
    def create_improved_data_loaders(self, train_data, val_data, batch_size=4, sequence_length=4):
        """ê°œì„ ëœ ë°ì´í„° ë¡œë” ìƒì„±"""
        print("ğŸ”„ ê°œì„ ëœ ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")
        
        # ì¦ê°• ì ìš©ì€ í›ˆë ¨ ë°ì´í„°ì—ë§Œ
        train_dataset = CloudDataset(
            [d['filepath'] for d in train_data],
            sequence_length=sequence_length,
        )
        
        val_dataset = CloudDataset(
            [d['filepath'] for d in val_data],
            sequence_length=sequence_length,
        )
        
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True, 
            num_workers=2,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        val_loader = DataLoader(
            val_dataset, 
            batch_size=batch_size, 
            shuffle=False, 
            num_workers=2,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        print(f"âœ… í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}ê°œ, ê²€ì¦ ë°ì´í„°: {len(val_dataset)}ê°œ")
        return train_loader, val_loader
    
    def train_model(self, train_loader, val_loader, num_epochs=100, device=None):
        """ê°œì„ ëœ ëª¨ë¸ í›ˆë ¨"""
        print("ğŸš€ ê°œì„ ëœ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        model = ImprovedCloudMovementPredictor(
            input_size=(256, 256),
            sequence_length=4,
            prediction_steps=1
        )
        
        if device is None:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            
        print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
        model = model.to(device)
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"ğŸ“Š ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
        print(f"ğŸ“Š í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
        
        train_losses, val_losses = improved_train_model(
            model, train_loader, val_loader,
            num_epochs=num_epochs, device=device
        )
        
        return model, train_losses, val_losses

# 5. ì‚¬ìš©ë²•
def run_improved_training_pipeline():
    """ê°œì„ ëœ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print("ğŸš€ ê°œì„ ëœ êµ¬ë¦„ ì´ë™ ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 60)
    
    # GPU/CPU ì„¤ì •
    if torch.cuda.is_available():
        device = torch.device('cuda:0')
        batch_size = 4  # ë©”ëª¨ë¦¬ ì•ˆì •ì„±ì„ ìœ„í•´ ì‘ê²Œ ì„¤ì •
        print("ğŸš€ GPU ëª¨ë“œë¡œ ì‹¤í–‰")
    else:
        device = torch.device('cpu')
        batch_size = 2
        print("âš ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    
    trainer = ImprovedCloudPredictionTrainer()
    
    try:
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° í™•ì¸
        processed_files = []
        if os.path.exists('./processed_cloud_data'):
            import glob
            processed_files = glob.glob('./processed_cloud_data/*_processed.pkl')
        
        if len(processed_files) < 10:
            print(f"âŒ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¶€ì¡±: {len(processed_files)}ê°œ")
            print("ë¨¼ì € gk2a_preprocessor.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return
        
        # í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í• 
        split_idx = int(len(processed_files) * 0.8)
        train_files = processed_files[:split_idx]
        val_files = processed_files[split_idx:]
        
        train_data = [{'filepath': f} for f in train_files]
        val_data = [{'filepath': f} for f in val_files]
        
        print(f"ğŸ“Š í›ˆë ¨ íŒŒì¼: {len(train_files)}ê°œ")
        print(f"ğŸ“Š ê²€ì¦ íŒŒì¼: {len(val_files)}ê°œ")
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        train_loader, val_loader = trainer.create_improved_data_loaders(
            train_data, val_data,
            batch_size=batch_size,
            sequence_length=4
        )
        
        # ëª¨ë¸ í›ˆë ¨
        model, train_losses, val_losses = trainer.train_improved_model(
            train_loader, val_loader,
            num_epochs=100,
            device=device
        )
        
        print("âœ… ê°œì„ ëœ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return

if __name__ == "__main__":
    run_improved_training_pipeline()
