import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import json
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import seaborn as sns

# ì•ì„œ ìƒì„±í•œ ëª¨ë“ˆë“¤ import
sys.path.append('.')
from gk2a_preprocessor import GK2ACloudProcessor
from cloud_convlstm_model import CloudMovementPredictor, CloudDataset, train_model


class CloudPredictionTrainer:
    """êµ¬ë¦„ ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ í´ë˜ìŠ¤"""

    def __init__(self, data_folder="./processed_cloud_data", model_save_dir="./models"):
        self.data_folder = data_folder
        self.model_save_dir = model_save_dir
        self.processor = GK2ACloudProcessor(data_folder)

        os.makedirs(model_save_dir, exist_ok=True)

    def prepare_training_data(self, start_date, end_date, train_ratio=0.8):
        """í›ˆë ¨ ë°ì´í„° ì¤€ë¹„"""
        print("ğŸ“Š í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

        # ì‹œê³„ì—´ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        file_list = self.processor.collect_time_series(start_date, end_date, interval_minutes=10)

        if len(file_list) < 10:
            raise ValueError(f"ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìµœì†Œ 10ê°œ íŒŒì¼ í•„ìš”, í˜„ì¬: {len(file_list)}ê°œ")

        print(f"âœ… ì´ {len(file_list)}ê°œ íŒŒì¼ ë°œê²¬")

        # ë°ì´í„° ê²€ì¦ ë° ì „ì²˜ë¦¬
        processed_data = []
        valid_files = []

        for i, filepath in enumerate(file_list):
            try:
                cloud_mask, metadata = self.processor.load_cloud_data(filepath)
                if cloud_mask is not None:
                    binary_mask = self.processor.preprocess_cloud_mask(cloud_mask)

                    # íŒ¨ì¹˜ë¡œ ë¶„í•  (256x256)
                    patches, positions = self.processor.create_image_patches(
                        binary_mask, patch_size=(256, 256), overlap=0.3
                    )

                    processed_data.append({
                        'filepath': filepath,
                        'patches': patches,
                        'positions': positions,
                        'timestamp': metadata.get('time', '')
                    })
                    valid_files.append(filepath)

                if (i + 1) % 10 == 0:
                    print(f"ì²˜ë¦¬ ì§„í–‰ë¥ : {i+1}/{len(file_list)} ({((i+1)/len(file_list)*100):.1f}%)")

            except Exception as e:
                print(f"âš ï¸  íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {filepath}: {e}")
                continue

        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_data)}ê°œ ì‹œê°„ëŒ€ ë°ì´í„°")

        # í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í• 
        split_idx = int(len(processed_data) * train_ratio)
        train_data = processed_data[:split_idx]
        val_data = processed_data[split_idx:]

        return train_data, val_data, valid_files

    def create_data_loaders(self, train_data, val_data, batch_size=8, sequence_length=4):
        """ë°ì´í„° ë¡œë” ìƒì„±"""
        print("ğŸ”„ ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")

        train_dataset = CloudDataset(
            [d['filepath'] for d in train_data], 
            sequence_length=sequence_length
        )
        val_dataset = CloudDataset(
            [d['filepath'] for d in val_data], 
            sequence_length=sequence_length
        )

        train_loader = torch.utils.data.DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True, num_workers=2
        )
        val_loader = torch.utils.data.DataLoader(
            val_dataset, batch_size=batch_size, shuffle=False, num_workers=2
        )

        print(f"âœ… í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}ê°œ, ê²€ì¦ ë°ì´í„°: {len(val_dataset)}ê°œ")

        return train_loader, val_loader

    def train_and_evaluate(self, train_loader, val_loader, num_epochs=50):
        """ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€"""
        print("ğŸš€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")

        model = CloudMovementPredictor(
            input_size=(256, 256),
            sequence_length=4,
            prediction_steps=1
        )

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

        model = model.to(device)

        train_losses, val_losses = train_model(
            model, train_loader, val_loader, 
            num_epochs=num_epochs, device=device
        )

        self.plot_training_history(train_losses, val_losses)

        final_model_path = os.path.join(self.model_save_dir, 'final_cloud_model.pth')
        torch.save(model.state_dict(), final_model_path)

        config = {
            'model_architecture': 'ConvLSTM',
            'input_size': [256, 256],
            'sequence_length': 4,
            'prediction_steps': 1,
            'num_epochs': num_epochs,
            'final_train_loss': train_losses[-1],
            'final_val_loss': val_losses[-1],
            'device': str(device),
            'timestamp': datetime.now().isoformat()
        }

        with open(os.path.join(self.model_save_dir, 'model_config.json'), 'w') as f:
            json.dump(config, f, indent=2)

        print(f"âœ… í›ˆë ¨ ì™„ë£Œ! ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")

        return model, train_losses, val_losses

    def evaluate_model(self, model, test_loader, device):
        """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")

        model.eval()
        all_predictions = []
        all_targets = []

        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)

                predictions = (output > 0.5).float()

                all_predictions.extend(predictions.cpu().numpy().flatten())
                all_targets.extend(target.cpu().numpy().flatten())

        accuracy = accuracy_score(all_targets, all_predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(
            all_targets, all_predictions, average='binary'
        )

        cm = confusion_matrix(all_targets, all_predictions)

        results = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'confusion_matrix': cm.tolist()
        }

        print(f"ğŸ“Š ì„±ëŠ¥ ê²°ê³¼:")
        print(f"   ì •í™•ë„ (Accuracy): {accuracy:.4f}")
        print(f"   ì •ë°€ë„ (Precision): {precision:.4f}")
        print(f"   ì¬í˜„ìœ¨ (Recall): {recall:.4f}")
        print(f"   F1-Score: {f1:.4f}")

        return results

    def plot_training_history(self, train_losses, val_losses):
        """í›ˆë ¨ ê³¼ì • ì‹œê°í™”"""
        plt.figure(figsize=(10, 6))
        plt.plot(train_losses, label='Train Loss', color='blue')
        plt.plot(val_losses, label='Validation Loss', color='red')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training History - Cloud Movement Prediction')
        plt.legend()
        plt.grid(True)

        save_path = os.path.join(self.model_save_dir, 'training_history.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

        print(f"âœ… í›ˆë ¨ ê³¼ì • ê·¸ë˜í”„ ì €ì¥: {save_path}")

def run_full_training_pipeline():
    print("ğŸš€ êµ¬ë¦„ ì´ë™ ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 60)

    trainer = CloudPredictionTrainer()

    try:
        end_date = datetime.now().strftime('%Y%m%d%H%M')
        start_date = (datetime.now() - timedelta(days=7)).strftime('%Y%m%d%H%M')

        print(f"ğŸ“… ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}")

        if not os.path.exists('./processed_cloud_data') or len(os.listdir('./processed_cloud_data')) < 10:
            print("âš ï¸  ì¶©ë¶„í•œ GK2A ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print("ë¨¼ì € gk2a_downloader.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”.")
            return

        # ì‹¤ì œ ë°ì´í„°ë¡œ ëŒ€ì²´
        train_data, val_data, valid_files = trainer.prepare_training_data(start_date, end_date)
        train_loader, val_loader = trainer.create_data_loaders(train_data, val_data, batch_size=2, sequence_length=4)

        model, train_losses, val_losses = trainer.train_and_evaluate(
            train_loader, val_loader, num_epochs=50  # ë³¸ê²© í•™ìŠµ ê¸°ê°„ ì¡°ì ˆ ê°€ëŠ¥
        )

        print("âœ… í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")

    except Exception as e:
        print(f"âŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return



if __name__ == "__main__":
    run_full_training_pipeline()
