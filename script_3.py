# ConvLSTM ëª¨ë¸ êµ¬í˜„ì„ ìœ„í•œ ì½”ë“œ í…œí”Œë¦¿ ìƒì„±
convlstm_code = '''
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import os
from datetime import datetime

class ConvLSTMCell(nn.Module):
    """ConvLSTM Cell êµ¬í˜„"""
    
    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):
        super(ConvLSTMCell, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.kernel_size = kernel_size
        self.padding = kernel_size[0] // 2, kernel_size[1] // 2
        self.bias = bias
        
        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,
                              out_channels=4 * self.hidden_dim,
                              kernel_size=self.kernel_size,
                              padding=self.padding,
                              bias=self.bias)

    def forward(self, input_tensor, cur_state):
        h_cur, c_cur = cur_state
        
        combined = torch.cat([input_tensor, h_cur], dim=1)
        combined_conv = self.conv(combined)
        
        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)
        i = torch.sigmoid(cc_i)
        f = torch.sigmoid(cc_f)
        o = torch.sigmoid(cc_o)
        g = torch.tanh(cc_g)

        c_next = f * c_cur + i * g
        h_next = o * torch.tanh(c_next)
        
        return h_next, c_next

class ConvLSTM(nn.Module):
    """ConvLSTM ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, input_dim, hidden_dims, kernel_sizes, num_layers, 
                 batch_first=True, bias=True, return_all_layers=False):
        super(ConvLSTM, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.kernel_sizes = kernel_sizes
        self.num_layers = num_layers
        self.batch_first = batch_first
        self.bias = bias
        self.return_all_layers = return_all_layers
        
        cell_list = []
        for i in range(self.num_layers):
            cur_input_dim = self.input_dim if i == 0 else self.hidden_dims[i-1]
            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,
                                        hidden_dim=self.hidden_dims[i],
                                        kernel_size=self.kernel_sizes[i],
                                        bias=self.bias))
        
        self.cell_list = nn.ModuleList(cell_list)

    def forward(self, input_tensor, hidden_state=None):
        if not self.batch_first:
            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)
        
        b, seq_len, _, h, w = input_tensor.size()
        
        if hidden_state is None:
            hidden_state = self._init_hidden(batch_size=b, image_size=(h, w))
        
        layer_output_list = []
        last_state_list = []
        
        cur_layer_input = input_tensor
        
        for layer_idx in range(self.num_layers):
            h, c = hidden_state[layer_idx]
            output_inner = []
            
            for t in range(seq_len):
                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],
                                                cur_state=[h, c])
                output_inner.append(h)
            
            layer_output = torch.stack(output_inner, dim=1)
            cur_layer_input = layer_output
            
            layer_output_list.append(layer_output)
            last_state_list.append([h, c])
        
        if not self.return_all_layers:
            layer_output_list = layer_output_list[-1:]
            last_state_list = last_state_list[-1:]
        
        return layer_output_list, last_state_list

    def _init_hidden(self, batch_size, image_size):
        init_states = []
        for i in range(self.num_layers):
            init_states.append(self.cell_list[i]._init_hidden(batch_size, image_size))
        return init_states

class CloudMovementPredictor(nn.Module):
    """êµ¬ë¦„ ì´ë™ ì˜ˆì¸¡ ëª¨ë¸"""
    
    def __init__(self, input_size=(256, 256), sequence_length=4, prediction_steps=1):
        super(CloudMovementPredictor, self).__init__()
        
        self.input_size = input_size
        self.sequence_length = sequence_length
        self.prediction_steps = prediction_steps
        
        # ConvLSTM ë ˆì´ì–´
        self.convlstm = ConvLSTM(
            input_dim=1,
            hidden_dims=[32, 32],
            kernel_sizes=[(3, 3), (3, 3)],
            num_layers=2,
            batch_first=True,
            bias=True,
            return_all_layers=False
        )
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.output_layer = nn.Sequential(
            nn.Conv2d(32, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, prediction_steps, kernel_size=1),
            nn.Sigmoid()  # êµ¬ë¦„ í™•ë¥  (0-1)
        )
        
    def forward(self, x):
        # x: (batch, sequence, channels, height, width)
        lstm_out, _ = self.convlstm(x)
        
        # ë§ˆì§€ë§‰ ì‹œê°„ ìŠ¤í…ì˜ ì¶œë ¥ ì‚¬ìš©
        last_output = lstm_out[0][:, -1, :, :, :]  # (batch, channels, height, width)
        
        # ì˜ˆì¸¡ ì¶œë ¥
        prediction = self.output_layer(last_output)
        
        return prediction

class CloudDataset(Dataset):
    """êµ¬ë¦„ ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    
    def __init__(self, data_paths, sequence_length=4, prediction_steps=1):
        self.data_paths = sorted(data_paths)
        self.sequence_length = sequence_length
        self.prediction_steps = prediction_steps
        
        # ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„±
        self.sequences = self._create_sequences()
    
    def _create_sequences(self):
        sequences = []
        for i in range(len(self.data_paths) - self.sequence_length - self.prediction_steps + 1):
            input_sequence = self.data_paths[i:i + self.sequence_length]
            target = self.data_paths[i + self.sequence_length:i + self.sequence_length + self.prediction_steps]
            sequences.append((input_sequence, target))
        return sequences
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        input_paths, target_paths = self.sequences[idx]
        
        # ì…ë ¥ ì‹œí€€ìŠ¤ ë¡œë“œ
        input_data = []
        for path in input_paths:
            # ì—¬ê¸°ì„œ ì‹¤ì œ ë°ì´í„° ë¡œë”© ë¡œì§ êµ¬í˜„
            # ì˜ˆ: cloud_mask = load_cloud_data(path)
            # ì„ì‹œë¡œ ë”ë¯¸ ë°ì´í„° ìƒì„±
            dummy_data = np.random.randint(0, 2, (256, 256)).astype(np.float32)
            input_data.append(dummy_data)
        
        # íƒ€ê²Ÿ ë°ì´í„° ë¡œë“œ
        target_data = []
        for path in target_paths:
            dummy_target = np.random.randint(0, 2, (256, 256)).astype(np.float32)
            target_data.append(dummy_target)
        
        input_tensor = torch.FloatTensor(input_data).unsqueeze(1)  # (seq, 1, H, W)
        target_tensor = torch.FloatTensor(target_data)  # (pred_steps, H, W)
        
        return input_tensor, target_tensor

def train_model(model, train_loader, val_loader, num_epochs=50, device='cuda'):
    """ëª¨ë¸ í›ˆë ¨ í•¨ìˆ˜"""
    
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)
    
    train_losses = []
    val_losses = []
    
    best_val_loss = float('inf')
    
    for epoch in range(num_epochs):
        # í›ˆë ¨ ëª¨ë“œ
        model.train()
        train_loss = 0.0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            
            # íƒ€ê²Ÿì„ (batch, pred_steps, H, W) í˜•íƒœë¡œ ë³€í™˜
            target = target.permute(1, 0, 2, 3)  # (pred_steps, batch, H, W) -> (batch, pred_steps, H, W)
            
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            
            if batch_idx % 10 == 0:
                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.6f}')
        
        # ê²€ì¦ ëª¨ë“œ
        model.eval()
        val_loss = 0.0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                target = target.permute(1, 0, 2, 3)
                loss = criterion(output, target)
                val_loss += loss.item()
        
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        
        scheduler.step(avg_val_loss)
        
        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}')
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), 'best_cloud_model.pth')
            print(f'âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (Val Loss: {best_val_loss:.6f})')
    
    return train_losses, val_losses

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = CloudMovementPredictor(
        input_size=(256, 256),
        sequence_length=4,
        prediction_steps=1
    )
    
    print(f"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    model = model.to(device)
    
    # ë”ë¯¸ ë°ì´í„°ë¡œ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    dummy_input = torch.randn(2, 4, 1, 256, 256).to(device)  # (batch, seq, channels, H, W)
    with torch.no_grad():
        output = model(dummy_input)
        print(f"âœ… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì¶œë ¥ í¬ê¸°: {output.shape}")
'''

# ConvLSTM ëª¨ë¸ ì½”ë“œ ì €ì¥
with open('cloud_convlstm_model.py', 'w', encoding='utf-8') as f:
    f.write(convlstm_code)

print("âœ… ConvLSTM ëª¨ë¸ ì½”ë“œ ìƒì„± ì™„ë£Œ: cloud_convlstm_model.py")
print("\nğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€:")
print("pip install torch torchvision scikit-learn")